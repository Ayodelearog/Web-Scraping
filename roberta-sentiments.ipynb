{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "import torch\n",
    "import re\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from pytorch checkpoint file for 'C:\\Users\\ACER/.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\bc2764f8af2e92b6eb5679868df33e224075ca68\\pytorch_model.bin' at 'C:\\Users\\ACER/.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\bc2764f8af2e92b6eb5679868df33e224075ca68\\pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:415\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 415\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(checkpoint_file, map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:815\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    814\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 815\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:1051\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1050\u001b[0m typed_storage \u001b[39m=\u001b[39m deserialized_objects[key]\n\u001b[1;32m-> 1051\u001b[0m typed_storage\u001b[39m.\u001b[39;49m_untyped_storage\u001b[39m.\u001b[39;49m_set_from_file(\n\u001b[0;32m   1052\u001b[0m     f, offset, f_should_read_directly,\n\u001b[0;32m   1053\u001b[0m     torch\u001b[39m.\u001b[39;49m_utils\u001b[39m.\u001b[39;49m_element_size(typed_storage\u001b[39m.\u001b[39;49mdtype))\n\u001b[0;32m   1054\u001b[0m \u001b[39mif\u001b[39;00m offset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: unexpected EOF, expected 2334937 more bytes. The file might be corrupted.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:419\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(checkpoint_file) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 419\u001b[0m     \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39;49mread(\u001b[39m7\u001b[39;49m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    420\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    421\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou seem to have cloned a repository without having git-lfs installed. Please install \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    422\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgit-lfs and run `git lfs install` followed by `git lfs pull` in the folder \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    423\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myou cloned.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    424\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39mcharmap_decode(\u001b[39minput\u001b[39m,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors,decoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 2221: character maps to <undefined>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m RobertaForSequenceClassification, RobertaTokenizer\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[39m=\u001b[39m RobertaTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mroberta-base\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m model \u001b[39m=\u001b[39m RobertaForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mroberta-base\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:2429\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2426\u001b[0m \u001b[39mif\u001b[39;00m from_pt:\n\u001b[0;32m   2427\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sharded \u001b[39mand\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2428\u001b[0m         \u001b[39m# Time to load the checkpoint\u001b[39;00m\n\u001b[1;32m-> 2429\u001b[0m         state_dict \u001b[39m=\u001b[39m load_state_dict(resolved_archive_file)\n\u001b[0;32m   2431\u001b[0m     \u001b[39m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[0;32m   2432\u001b[0m     \u001b[39m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[0;32m   2433\u001b[0m     \u001b[39m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[0;32m   2434\u001b[0m     \u001b[39m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001b[39;00m\n\u001b[0;32m   2435\u001b[0m     \u001b[39m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[0;32m   2436\u001b[0m     dtype_orig \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:431\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    426\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    427\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to locate the file \u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_file\u001b[39m}\u001b[39;00m\u001b[39m which is necessary to load this pretrained \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmodel. Make sure you have saved the model properly.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m             ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mUnicodeDecodeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m--> 431\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    432\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to load weights from pytorch checkpoint file for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    433\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mat \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    434\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    435\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for 'C:\\Users\\ACER/.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\bc2764f8af2e92b6eb5679868df33e224075ca68\\pytorch_model.bin' at 'C:\\Users\\ACER/.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\bc2764f8af2e92b6eb5679868df33e224075ca68\\pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
     ]
    }
   ],
   "source": [
    "# # lod pre-trained Roberta model\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = sntwitter.TwitterSearchScraper(\"mkbhd: iPhone\")\n",
    "\n",
    "tweets = []\n",
    "\n",
    "# for i, tweet in enumerate(scraper.get_items()):\n",
    "#     data = [\n",
    "#         tweet.date,\n",
    "#         tweet.id,\n",
    "#         tweet.rawContent,\n",
    "#         tweet.user.username,\n",
    "#         tweet.likeCount,\n",
    "#         tweet.retweetCount,\n",
    "#     ]\n",
    "#     tweets.append(data)\n",
    "#     if i > 150:\n",
    "#         break\n",
    "\n",
    "for i, tweet in enumerate(scraper.get_items()):\n",
    "    data = [\n",
    "        tweet.date,\n",
    "        # tweet.id,\n",
    "        tweet.rawContent,\n",
    "        tweet.user.username,\n",
    "        # tweet.likeCount,\n",
    "        # tweet.retweetCount,\n",
    "    ]\n",
    "    tweets.append(data)\n",
    "    if i > 150:\n",
    "        break\n",
    "\n",
    "# print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.DataFrame(tweets, columns = [\"date\", \"text\", \"username\"])\n",
    "\n",
    "# print(tweet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "def preprocess(df):\n",
    "    inputs = []\n",
    "    for tweet in df['text']:\n",
    "        # Clean the tweet by removing mentions and links\n",
    "        tweet = re.sub(r\"@[A-Za-z0-9]+\", \"\", tweet)  # remove mentions\n",
    "        tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", \"\", tweet)  # remove links\n",
    "        \n",
    "        # Tokenize the cleaned tweet and convert it to input tensors\n",
    "        # encoded_tweet = tokenizer(tweet, return_tensors = \"pt\")\n",
    "        encoding = tokenizer(tweet, return_tensors='pt', padding=True, truncation=True)\n",
    "        # input_ids = encoding['input_ids']\n",
    "        # attention_mask = encoding['attention_mask']\n",
    "        # inputs.append((input_ids, attention_mask))\n",
    "        inputs.append(encoding)\n",
    "    return inputs\n",
    "\n",
    "inputs = preprocess(tweet_df)\n",
    "# print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download model and tokenizer\n",
    "roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
    "tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "\n",
    "labels = [\"Negative\", \"Neutral\", \"Positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.7245386 , 0.24091867, 0.0345427 ], dtype=float32), array([0.7193861 , 0.25331813, 0.02729579], dtype=float32), array([0.1138112 , 0.43998063, 0.44620815], dtype=float32), array([0.05892144, 0.25102964, 0.6900489 ], dtype=float32), array([0.94087076, 0.05303213, 0.00609712], dtype=float32), array([0.00307558, 0.04572709, 0.95119727], dtype=float32), array([0.08998917, 0.519544  , 0.39046684], dtype=float32), array([0.88326055, 0.09838251, 0.01835697], dtype=float32), array([0.7395638 , 0.20523669, 0.05519952], dtype=float32), array([0.00406792, 0.1280718 , 0.86786026], dtype=float32), array([0.00399771, 0.27198815, 0.7240141 ], dtype=float32), array([0.02779398, 0.19012351, 0.78208256], dtype=float32), array([0.13260987, 0.58834016, 0.2790499 ], dtype=float32), array([0.00392496, 0.3283613 , 0.66771376], dtype=float32), array([0.01936672, 0.17961752, 0.8010157 ], dtype=float32), array([0.00275458, 0.19747342, 0.79977196], dtype=float32), array([0.00572162, 0.16192919, 0.8323492 ], dtype=float32), array([0.31689155, 0.32603753, 0.35707092], dtype=float32), array([0.00414225, 0.0824159 , 0.9134419 ], dtype=float32), array([0.01851412, 0.263825  , 0.7176609 ], dtype=float32), array([0.14382392, 0.6783324 , 0.17784362], dtype=float32), array([0.00365793, 0.08882126, 0.90752083], dtype=float32), array([0.00427028, 0.5864927 , 0.409237  ], dtype=float32), array([0.00216364, 0.13775046, 0.8600859 ], dtype=float32), array([0.00501851, 0.13600278, 0.8589787 ], dtype=float32), array([0.7473144 , 0.23063765, 0.02204795], dtype=float32), array([0.1358096 , 0.8189367 , 0.04525376], dtype=float32), array([0.275841  , 0.64954454, 0.07461447], dtype=float32), array([0.00254374, 0.01759963, 0.97985655], dtype=float32), array([0.00119892, 0.02091692, 0.9778842 ], dtype=float32), array([0.01712591, 0.09951262, 0.88336146], dtype=float32), array([0.02347436, 0.7025595 , 0.27396622], dtype=float32), array([0.7658752 , 0.19116046, 0.04296428], dtype=float32), array([0.85890585, 0.12806746, 0.01302669], dtype=float32), array([0.09682399, 0.35366905, 0.54950696], dtype=float32), array([0.21712002, 0.6985816 , 0.08429838], dtype=float32), array([7.7820866e-04, 3.1158898e-02, 9.6806294e-01], dtype=float32), array([0.5154213 , 0.34139365, 0.14318502], dtype=float32), array([0.8524873 , 0.12971601, 0.01779672], dtype=float32), array([0.3238154 , 0.59165037, 0.08453418], dtype=float32), array([0.04552171, 0.2480359 , 0.7064424 ], dtype=float32), array([0.21704409, 0.7250231 , 0.05793285], dtype=float32), array([0.0193706, 0.6585279, 0.3221015], dtype=float32), array([0.00151507, 0.09439294, 0.90409195], dtype=float32), array([0.00212254, 0.01220554, 0.9856719 ], dtype=float32), array([0.5373604 , 0.32133138, 0.14130816], dtype=float32), array([0.02904129, 0.23812403, 0.7328347 ], dtype=float32), array([0.00524933, 0.18695374, 0.80779696], dtype=float32), array([0.00139762, 0.12460282, 0.8739996 ], dtype=float32), array([0.06417059, 0.85003346, 0.08579599], dtype=float32), array([0.7790604 , 0.20132644, 0.01961314], dtype=float32), array([0.67416847, 0.28325844, 0.0425731 ], dtype=float32), array([0.03323727, 0.3601615 , 0.6066012 ], dtype=float32), array([0.02051382, 0.2861321 , 0.69335407], dtype=float32), array([0.7441432 , 0.20944028, 0.04641648], dtype=float32), array([0.00128631, 0.1130636 , 0.88565016], dtype=float32), array([0.8564643 , 0.13231714, 0.01121858], dtype=float32), array([0.03045156, 0.80999696, 0.15955143], dtype=float32), array([0.00681999, 0.0987231 , 0.8944569 ], dtype=float32), array([0.02395676, 0.8121746 , 0.16386862], dtype=float32), array([0.00198068, 0.04300597, 0.95501333], dtype=float32), array([0.2623183 , 0.4937951 , 0.24388658], dtype=float32), array([0.00772598, 0.29901522, 0.6932588 ], dtype=float32), array([0.7464415 , 0.23644821, 0.01711031], dtype=float32), array([0.55885184, 0.33892345, 0.10222468], dtype=float32), array([0.09992194, 0.8376587 , 0.06241943], dtype=float32), array([0.23269849, 0.5251117 , 0.24218981], dtype=float32), array([0.2123988 , 0.51643133, 0.2711699 ], dtype=float32), array([0.06467018, 0.33867148, 0.59665835], dtype=float32), array([0.09313837, 0.668405  , 0.23845659], dtype=float32), array([0.00163854, 0.01443179, 0.98392975], dtype=float32), array([0.00189292, 0.05330032, 0.9448068 ], dtype=float32), array([0.26401383, 0.49851513, 0.23747109], dtype=float32), array([0.00299634, 0.02977778, 0.9672259 ], dtype=float32), array([0.0016269 , 0.02431701, 0.97405607], dtype=float32), array([0.24819787, 0.7132776 , 0.03852457], dtype=float32), array([0.06923335, 0.8239015 , 0.10686518], dtype=float32), array([0.6955509, 0.2704472, 0.0340018], dtype=float32), array([0.00323926, 0.12402214, 0.8727386 ], dtype=float32), array([0.01247196, 0.17061529, 0.81691283], dtype=float32), array([0.7289247 , 0.25044826, 0.02062702], dtype=float32), array([0.02307569, 0.12292659, 0.85399777], dtype=float32), array([0.13354672, 0.8275992 , 0.03885403], dtype=float32), array([0.9155438 , 0.08023559, 0.00422061], dtype=float32), array([0.02803786, 0.7694666 , 0.20249557], dtype=float32), array([0.64926314, 0.31297517, 0.03776174], dtype=float32), array([0.95000684, 0.04688323, 0.00310996], dtype=float32), array([0.0071139, 0.4246107, 0.5682754], dtype=float32), array([0.8964949 , 0.09412897, 0.00937604], dtype=float32), array([0.62376964, 0.3155489 , 0.06068147], dtype=float32), array([0.04604365, 0.6932387 , 0.26071763], dtype=float32), array([0.0025858 , 0.08214893, 0.9152653 ], dtype=float32), array([0.02663486, 0.6410418 , 0.33232334], dtype=float32), array([0.02651346, 0.35717693, 0.61630964], dtype=float32), array([0.0477219 , 0.44428575, 0.5079924 ], dtype=float32), array([0.00910657, 0.5579466 , 0.4329468 ], dtype=float32), array([0.19042486, 0.49515373, 0.3144214 ], dtype=float32), array([0.00514609, 0.37644443, 0.6184095 ], dtype=float32), array([0.00162181, 0.00991988, 0.98845834], dtype=float32), array([0.01439143, 0.7599878 , 0.22562078], dtype=float32), array([0.01259443, 0.85966593, 0.1277397 ], dtype=float32), array([0.00885074, 0.27976102, 0.7113883 ], dtype=float32), array([0.00643318, 0.3004556 , 0.69311124], dtype=float32), array([0.00381916, 0.39943263, 0.59674823], dtype=float32), array([0.04551811, 0.41446185, 0.54002005], dtype=float32), array([0.02315039, 0.84388095, 0.1329686 ], dtype=float32), array([0.08750001, 0.37492067, 0.5375793 ], dtype=float32), array([0.00171551, 0.03317038, 0.9651141 ], dtype=float32), array([0.00263588, 0.33049163, 0.6668725 ], dtype=float32), array([0.24011314, 0.43890047, 0.32098636], dtype=float32), array([0.00748665, 0.6800268 , 0.3124866 ], dtype=float32), array([0.8940058 , 0.09550249, 0.0104918 ], dtype=float32), array([0.01203304, 0.22925395, 0.758713  ], dtype=float32), array([0.2820142 , 0.640869  , 0.07711672], dtype=float32), array([0.27969876, 0.61620754, 0.10409372], dtype=float32), array([0.09406613, 0.37298208, 0.5329518 ], dtype=float32), array([0.0262223 , 0.6642054 , 0.30957237], dtype=float32), array([0.01635266, 0.6248516 , 0.35879576], dtype=float32), array([0.0359568 , 0.66842747, 0.29561576], dtype=float32), array([0.637081  , 0.33319718, 0.02972182], dtype=float32), array([0.03893877, 0.4163096 , 0.54475164], dtype=float32), array([0.00120356, 0.01794433, 0.98085207], dtype=float32), array([0.47713012, 0.4127317 , 0.11013816], dtype=float32), array([0.00116102, 0.05807207, 0.94076693], dtype=float32), array([0.04700033, 0.21891703, 0.73408264], dtype=float32), array([0.19929941, 0.6653224 , 0.13537815], dtype=float32), array([0.30954602, 0.58242106, 0.10803289], dtype=float32), array([0.5158376 , 0.37686715, 0.10729522], dtype=float32), array([0.03982638, 0.19369055, 0.766483  ], dtype=float32), array([0.1079094, 0.4390163, 0.4530743], dtype=float32), array([0.04005598, 0.4124551 , 0.54748887], dtype=float32), array([0.0885496 , 0.43368652, 0.47776383], dtype=float32), array([0.12597671, 0.48949918, 0.38452417], dtype=float32), array([0.03566989, 0.43654516, 0.52778494], dtype=float32), array([0.38053373, 0.48191625, 0.13754994], dtype=float32), array([0.00516938, 0.47775763, 0.517073  ], dtype=float32), array([0.00570143, 0.623235  , 0.37106356], dtype=float32), array([0.02072119, 0.2576236 , 0.7216552 ], dtype=float32), array([0.11883435, 0.5413837 , 0.339782  ], dtype=float32), array([0.06759772, 0.29992706, 0.63247526], dtype=float32), array([0.7583803 , 0.18978347, 0.05183621], dtype=float32), array([0.04700074, 0.19174261, 0.76125664], dtype=float32), array([0.00335192, 0.19057906, 0.806069  ], dtype=float32), array([0.002721  , 0.01626633, 0.9810127 ], dtype=float32), array([0.00399244, 0.1189218 , 0.8770858 ], dtype=float32), array([0.00258823, 0.02303054, 0.97438127], dtype=float32), array([0.6079664 , 0.31662953, 0.07540407], dtype=float32), array([0.0361027 , 0.31841454, 0.6454827 ], dtype=float32), array([0.00223513, 0.00629984, 0.99146503], dtype=float32), array([0.07460944, 0.42232952, 0.503061  ], dtype=float32), array([0.6775418 , 0.2935806 , 0.02887758], dtype=float32), array([0.00226958, 0.00783039, 0.98990005], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis\n",
    "sentiment_results = []\n",
    "for input in inputs:\n",
    "    # run through roberta sentiment analysis model\n",
    "    output = model(**input)\n",
    "\n",
    "    # convert to probability so score are only between 0 and 1\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    # print(scores)\n",
    "\n",
    "    # for i in range(len(scores)):\n",
    "    #     l = labels[i]\n",
    "    #     s = scores[i]\n",
    "\n",
    "    sentiment_results.append(scores)\n",
    "\n",
    "\n",
    "print(sentiment_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7245386, 0.7193861, 0.94087076, 0.88326055, 0.7395638, 0.7473144, 0.7658752, 0.85890585, 0.5154213, 0.8524873, 0.5373604, 0.7790604, 0.67416847, 0.7441432, 0.8564643, 0.7464415, 0.55885184, 0.6955509, 0.7289247, 0.9155438, 0.64926314, 0.95000684, 0.8964949, 0.62376964, 0.8940058, 0.637081, 0.47713012, 0.5158376, 0.7583803, 0.6079664, 0.6775418], [0.8189367, 0.64954454, 0.6985816, 0.59165037, 0.7250231, 0.4937951, 0.8376587, 0.49851513, 0.7132776, 0.8275992, 0.640869, 0.61620754, 0.6653224, 0.58242106, 0.48191625], [0.44620815, 0.6900489, 0.95119727, 0.86786026, 0.7240141, 0.78208256, 0.66771376, 0.8010157, 0.79977196, 0.8323492, 0.35707092, 0.9134419, 0.7176609, 0.90752083, 0.8600859, 0.8589787, 0.97985655, 0.9778842, 0.88336146, 0.54950696, 0.96806294, 0.7064424, 0.90409195, 0.9856719, 0.7328347, 0.80779696, 0.8739996, 0.6066012, 0.69335407, 0.88565016, 0.8944569, 0.95501333, 0.6932588, 0.59665835, 0.98392975, 0.9448068, 0.9672259, 0.97405607, 0.8727386, 0.81691283, 0.85399777, 0.5682754, 0.9152653, 0.61630964, 0.5079924, 0.6184095, 0.98845834, 0.7113883, 0.69311124, 0.59674823, 0.54002005, 0.5375793, 0.9651141, 0.6668725, 0.758713, 0.5329518, 0.54475164, 0.98085207, 0.94076693, 0.73408264, 0.766483, 0.4530743, 0.54748887, 0.47776383, 0.52778494, 0.517073, 0.7216552, 0.63247526, 0.76125664, 0.806069, 0.9810127, 0.8770858, 0.97438127, 0.6454827, 0.99146503, 0.503061, 0.98990005]]\n"
     ]
    }
   ],
   "source": [
    "# create variables to separate results of each tweet. if it is negative, neutral or porsitive.\n",
    "negative_results = []\n",
    "neutral_results = []\n",
    "positive_results = []\n",
    "\n",
    "# loop through the sentiment_results list and filter out the results according to its label index. index 0 for negative, 1 for neutral, 2 for positive \n",
    "for result in sentiment_results:\n",
    "    if result[0] > result[1] > result[2]:\n",
    "        negative_results.append(result[0])\n",
    "    elif result[1] > result[0] > result[2]:\n",
    "        neutral_results.append(result[1])\n",
    "    if result[2] > result[1] > result[0]:\n",
    "        positive_results.append(result[2])\n",
    "        \n",
    "\n",
    "results_by_label = []\n",
    "\n",
    "results_by_label.append(negative_results)\n",
    "results_by_label.append(neutral_results)\n",
    "results_by_label.append(positive_results)\n",
    "\n",
    "\n",
    "\n",
    "# print(results_by_label[0])\n",
    "# print(results_by_label[1])\n",
    "# print(results_by_label[2])\n",
    "\n",
    "# final_results = [{\"Negative\": negative_results,\n",
    "#                   \"Neutral\": neutral_results,\n",
    "#                   \"Positive\": positive_results}]\n",
    "\n",
    "print(results_by_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"Negative\": len(negative_results), \"Neutral\": len(neutral_results), \"Positive\": len(positive_results)}\n",
    "\n",
    "sentiment_results_df = list(data.keys())\n",
    "values = list(data.values())\n",
    "\n",
    "# sentiment_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 3 artists>"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "plt.bar(sentiment_results_df, values, color = \"maroon\", width = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis\n",
    "# sentiments = []\n",
    "# for input_ids, attention_mask in tqdm(inputs):\n",
    "#     outputs = model(input_ids, attention_mask=attention_mask)\n",
    "#     logits = outputs.logits\n",
    "#     sentiment = torch.argmax(logits, dim=1).item()\n",
    "#     sentiments.append(sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.724539</td>\n",
       "      <td>[0.8189367, 0.64954454, 0.6985816, 0.59165037,...</td>\n",
       "      <td>0.446208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.724539</td>\n",
       "      <td>[0.8189367, 0.64954454, 0.6985816, 0.59165037,...</td>\n",
       "      <td>0.690049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.724539</td>\n",
       "      <td>[0.8189367, 0.64954454, 0.6985816, 0.59165037,...</td>\n",
       "      <td>0.951197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.724539</td>\n",
       "      <td>[0.8189367, 0.64954454, 0.6985816, 0.59165037,...</td>\n",
       "      <td>0.86786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.724539</td>\n",
       "      <td>[0.8189367, 0.64954454, 0.6985816, 0.59165037,...</td>\n",
       "      <td>0.724014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2382</th>\n",
       "      <td>0.677542</td>\n",
       "      <td>[0.8189367, 0.64954454, 0.6985816, 0.59165037,...</td>\n",
       "      <td>0.974381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2383</th>\n",
       "      <td>0.677542</td>\n",
       "      <td>[0.8189367, 0.64954454, 0.6985816, 0.59165037,...</td>\n",
       "      <td>0.645483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>0.677542</td>\n",
       "      <td>[0.8189367, 0.64954454, 0.6985816, 0.59165037,...</td>\n",
       "      <td>0.991465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>0.677542</td>\n",
       "      <td>[0.8189367, 0.64954454, 0.6985816, 0.59165037,...</td>\n",
       "      <td>0.503061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2386</th>\n",
       "      <td>0.677542</td>\n",
       "      <td>[0.8189367, 0.64954454, 0.6985816, 0.59165037,...</td>\n",
       "      <td>0.9899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2387 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Negative                                            Neutral  Positive\n",
       "0     0.724539  [0.8189367, 0.64954454, 0.6985816, 0.59165037,...  0.446208\n",
       "1     0.724539  [0.8189367, 0.64954454, 0.6985816, 0.59165037,...  0.690049\n",
       "2     0.724539  [0.8189367, 0.64954454, 0.6985816, 0.59165037,...  0.951197\n",
       "3     0.724539  [0.8189367, 0.64954454, 0.6985816, 0.59165037,...   0.86786\n",
       "4     0.724539  [0.8189367, 0.64954454, 0.6985816, 0.59165037,...  0.724014\n",
       "...        ...                                                ...       ...\n",
       "2382  0.677542  [0.8189367, 0.64954454, 0.6985816, 0.59165037,...  0.974381\n",
       "2383  0.677542  [0.8189367, 0.64954454, 0.6985816, 0.59165037,...  0.645483\n",
       "2384  0.677542  [0.8189367, 0.64954454, 0.6985816, 0.59165037,...  0.991465\n",
       "2385  0.677542  [0.8189367, 0.64954454, 0.6985816, 0.59165037,...  0.503061\n",
       "2386  0.677542  [0.8189367, 0.64954454, 0.6985816, 0.59165037,...    0.9899\n",
       "\n",
       "[2387 rows x 3 columns]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_3 = sentiment_results_df.explode('Positive').reset_index(drop=True)\n",
    "sentiment_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.724539</td>\n",
       "      <td>0.818937</td>\n",
       "      <td>[0.44620815, 0.6900489, 0.95119727, 0.86786026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.724539</td>\n",
       "      <td>0.649545</td>\n",
       "      <td>[0.44620815, 0.6900489, 0.95119727, 0.86786026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.724539</td>\n",
       "      <td>0.698582</td>\n",
       "      <td>[0.44620815, 0.6900489, 0.95119727, 0.86786026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.724539</td>\n",
       "      <td>0.59165</td>\n",
       "      <td>[0.44620815, 0.6900489, 0.95119727, 0.86786026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.724539</td>\n",
       "      <td>0.725023</td>\n",
       "      <td>[0.44620815, 0.6900489, 0.95119727, 0.86786026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>0.677542</td>\n",
       "      <td>0.640869</td>\n",
       "      <td>[0.44620815, 0.6900489, 0.95119727, 0.86786026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>0.677542</td>\n",
       "      <td>0.616208</td>\n",
       "      <td>[0.44620815, 0.6900489, 0.95119727, 0.86786026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>0.677542</td>\n",
       "      <td>0.665322</td>\n",
       "      <td>[0.44620815, 0.6900489, 0.95119727, 0.86786026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>0.677542</td>\n",
       "      <td>0.582421</td>\n",
       "      <td>[0.44620815, 0.6900489, 0.95119727, 0.86786026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>0.677542</td>\n",
       "      <td>0.481916</td>\n",
       "      <td>[0.44620815, 0.6900489, 0.95119727, 0.86786026...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>465 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Negative   Neutral                                           Positive\n",
       "0    0.724539  0.818937  [0.44620815, 0.6900489, 0.95119727, 0.86786026...\n",
       "1    0.724539  0.649545  [0.44620815, 0.6900489, 0.95119727, 0.86786026...\n",
       "2    0.724539  0.698582  [0.44620815, 0.6900489, 0.95119727, 0.86786026...\n",
       "3    0.724539   0.59165  [0.44620815, 0.6900489, 0.95119727, 0.86786026...\n",
       "4    0.724539  0.725023  [0.44620815, 0.6900489, 0.95119727, 0.86786026...\n",
       "..        ...       ...                                                ...\n",
       "460  0.677542  0.640869  [0.44620815, 0.6900489, 0.95119727, 0.86786026...\n",
       "461  0.677542  0.616208  [0.44620815, 0.6900489, 0.95119727, 0.86786026...\n",
       "462  0.677542  0.665322  [0.44620815, 0.6900489, 0.95119727, 0.86786026...\n",
       "463  0.677542  0.582421  [0.44620815, 0.6900489, 0.95119727, 0.86786026...\n",
       "464  0.677542  0.481916  [0.44620815, 0.6900489, 0.95119727, 0.86786026...\n",
       "\n",
       "[465 rows x 3 columns]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_3 = sentiment_3.explode('Neutral').reset_index(drop=True)\n",
    "sentiment_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.724539</td>\n",
       "      <td>0.818937</td>\n",
       "      <td>0.446208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.724539</td>\n",
       "      <td>0.818937</td>\n",
       "      <td>0.690049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.724539</td>\n",
       "      <td>0.818937</td>\n",
       "      <td>0.951197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.724539</td>\n",
       "      <td>0.818937</td>\n",
       "      <td>0.86786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.724539</td>\n",
       "      <td>0.818937</td>\n",
       "      <td>0.724014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35800</th>\n",
       "      <td>0.677542</td>\n",
       "      <td>0.481916</td>\n",
       "      <td>0.974381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35801</th>\n",
       "      <td>0.677542</td>\n",
       "      <td>0.481916</td>\n",
       "      <td>0.645483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35802</th>\n",
       "      <td>0.677542</td>\n",
       "      <td>0.481916</td>\n",
       "      <td>0.991465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35803</th>\n",
       "      <td>0.677542</td>\n",
       "      <td>0.481916</td>\n",
       "      <td>0.503061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35804</th>\n",
       "      <td>0.677542</td>\n",
       "      <td>0.481916</td>\n",
       "      <td>0.9899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35805 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Negative   Neutral  Positive\n",
       "0      0.724539  0.818937  0.446208\n",
       "1      0.724539  0.818937  0.690049\n",
       "2      0.724539  0.818937  0.951197\n",
       "3      0.724539  0.818937   0.86786\n",
       "4      0.724539  0.818937  0.724014\n",
       "...         ...       ...       ...\n",
       "35800  0.677542  0.481916  0.974381\n",
       "35801  0.677542  0.481916  0.645483\n",
       "35802  0.677542  0.481916  0.991465\n",
       "35803  0.677542  0.481916  0.503061\n",
       "35804  0.677542  0.481916    0.9899\n",
       "\n",
       "[35805 rows x 3 columns]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_results_df = sentiment_results_df.reset_index(drop = True)\n",
    "sentiment_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_results_df = sentiment_results_df.explode('Neutral', 'Positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.724539</td>\n",
       "      <td>0.818937</td>\n",
       "      <td>[0.44620815, 0.6900489, 0.95119727, 0.86786026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.724539</td>\n",
       "      <td>0.649545</td>\n",
       "      <td>[0.44620815, 0.6900489, 0.95119727, 0.86786026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.724539</td>\n",
       "      <td>0.698582</td>\n",
       "      <td>[0.44620815, 0.6900489, 0.95119727, 0.86786026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.724539</td>\n",
       "      <td>0.59165</td>\n",
       "      <td>[0.44620815, 0.6900489, 0.95119727, 0.86786026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.724539</td>\n",
       "      <td>0.725023</td>\n",
       "      <td>[0.44620815, 0.6900489, 0.95119727, 0.86786026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>0.677542</td>\n",
       "      <td>0.640869</td>\n",
       "      <td>[0.44620815, 0.6900489, 0.95119727, 0.86786026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>0.677542</td>\n",
       "      <td>0.616208</td>\n",
       "      <td>[0.44620815, 0.6900489, 0.95119727, 0.86786026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>0.677542</td>\n",
       "      <td>0.665322</td>\n",
       "      <td>[0.44620815, 0.6900489, 0.95119727, 0.86786026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>0.677542</td>\n",
       "      <td>0.582421</td>\n",
       "      <td>[0.44620815, 0.6900489, 0.95119727, 0.86786026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>0.677542</td>\n",
       "      <td>0.481916</td>\n",
       "      <td>[0.44620815, 0.6900489, 0.95119727, 0.86786026...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>465 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Negative   Neutral                                           Positive\n",
       "0    0.724539  0.818937  [0.44620815, 0.6900489, 0.95119727, 0.86786026...\n",
       "1    0.724539  0.649545  [0.44620815, 0.6900489, 0.95119727, 0.86786026...\n",
       "2    0.724539  0.698582  [0.44620815, 0.6900489, 0.95119727, 0.86786026...\n",
       "3    0.724539   0.59165  [0.44620815, 0.6900489, 0.95119727, 0.86786026...\n",
       "4    0.724539  0.725023  [0.44620815, 0.6900489, 0.95119727, 0.86786026...\n",
       "..        ...       ...                                                ...\n",
       "460  0.677542  0.640869  [0.44620815, 0.6900489, 0.95119727, 0.86786026...\n",
       "461  0.677542  0.616208  [0.44620815, 0.6900489, 0.95119727, 0.86786026...\n",
       "462  0.677542  0.665322  [0.44620815, 0.6900489, 0.95119727, 0.86786026...\n",
       "463  0.677542  0.582421  [0.44620815, 0.6900489, 0.95119727, 0.86786026...\n",
       "464  0.677542  0.481916  [0.44620815, 0.6900489, 0.95119727, 0.86786026...\n",
       "\n",
       "[465 rows x 3 columns]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_results_df = sentiment_results_df.reset_index(drop = True)\n",
    "sentiment_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download model and tokenizer\n",
    "roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
    "tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "\n",
    "labels = [\"Negative\", \"Neutral\", \"Positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Sentiment analysis\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m encoded_tweet \u001b[39m=\u001b[39m tokenizer(inputs, return_tensors \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m \u001b[39m# output = model(encoded_tweet[\"input_ids\"], encoded_tweet[\"attention_mask\"])\u001b[39;00m\n\u001b[0;32m      4\u001b[0m output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoded_tweet)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:2530\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2528\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2529\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2530\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[0;32m   2531\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2532\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:2588\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2585\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   2587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[1;32m-> 2588\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2589\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2590\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2591\u001b[0m     )\n\u001b[0;32m   2593\u001b[0m \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[0;32m   2594\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2595\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2596\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2597\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "# Sentiment analysis\n",
    "encoded_tweet = tokenizer(inputs, return_tensors = \"pt\")\n",
    "# output = model(encoded_tweet[\"input_ids\"], encoded_tweet[\"attention_mask\"])\n",
    "output = model(**encoded_tweet)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    l = labels[i]\n",
    "    s = scores[i]\n",
    "    print(l, s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
